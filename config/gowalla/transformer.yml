misc:
  dataset: gowalla
  # 95567 known locations, ranging from 2 to 95568
  total_loc_num: 95569

  # 10560 users, ranging from 1 to 10560
  total_user_num: 10561

  # 
  if_embed_user: True
  if_embed_poi: False
  if_embed_time: True
  if_embed_duration: False

  # how many days in the past we consider
  previous_day: 7
  verbose: True # If True, enables more detailed logging during training.
  debug: False
  batch_size: 64
  print_step: 10 # Number of steps before printing metrics during training.
  num_workers: 0 # Number of processes used to load data (0 = single thread).
  # day_selection: default
  day_selection: default

embedding:
  base_emb_size: 128
  poi_original_size: 0

model:
  networkName: transformer # Specifies the model architecture (in this case, Transformer)
  # only for transformer
  num_encoder_layers: 4
  nhead: 8
  dim_feedforward: 256
  fc_dropout: 0.1 # Dropout rate (0.1) to prevent overfitting

optimiser:
  optimizer: Adam
  max_epoch: 100
  lr: 0.001
  weight_decay: 0.000001
  # lr: 0.01
  # for Adam
  beta1: 0.9
  beta2: 0.999
  # for SGD
  momentum: 0.98
  # for warmup
  num_warmup_epochs: 2
  num_training_epochs: 50
  # for decay
  patience: 2 # If the validation loss doesnâ€™t improve for 2 epochs, reduce the learning rate.
  lr_step_size: 1 # Number of epochs between each learning rate adjustment (1).
  lr_gamma: 0.1 # The factor by which the learning rate is reduced (0.1).

dataset:
  source_root: ./data/
  save_root: ./outputs/
